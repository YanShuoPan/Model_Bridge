"""
é ˜åŸŸæ¨è–¦ç³»çµ±å–®å…ƒæ¸¬è©¦

æ¸¬è©¦ Phase 1 çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1. é ˜åŸŸé…ç½®è¼‰å…¥
2. GPT é ˜åŸŸè­˜åˆ¥
3. æ–¹æ³• metadata è¼‰å…¥
4. å¤šé ˜åŸŸåŒ¹é…æ¨è–¦
"""

import sys
from pathlib import Path

# æ·»åŠ  backend åˆ°è·¯å¾‘
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from services.domain_service import (
    load_domains_config,
    get_all_domains,
    get_domain_info,
    validate_domain_id,
    load_all_methods_metadata,
    get_methods_by_domain
)
from services.recommender import recommend_methods_by_domains


def test_1_load_domains_config():
    """æ¸¬è©¦é ˜åŸŸé…ç½®è¼‰å…¥"""
    print("\n" + "=" * 60)
    print("æ¸¬è©¦ 1: è¼‰å…¥é ˜åŸŸé…ç½®")
    print("=" * 60)

    config = load_domains_config()

    assert "domains" in config, "é…ç½®æ‡‰åŒ…å« domains æ¬„ä½"
    assert len(config["domains"]) > 0, "æ‡‰è©²æœ‰è‡³å°‘ä¸€å€‹é ˜åŸŸ"

    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(config['domains'])} å€‹é ˜åŸŸ")

    # åˆ—å‡ºæ‰€æœ‰é ˜åŸŸ
    for domain_id, domain_info in config["domains"].items():
        print(f"   - {domain_id}: {domain_info.get('name', 'N/A')} ({domain_info.get('name_en', 'N/A')})")

    return True


def test_2_validate_domain_ids():
    """æ¸¬è©¦é ˜åŸŸIDé©—è­‰"""
    print("\n" + "=" * 60)
    print("æ¸¬è©¦ 2: é ˜åŸŸIDé©—è­‰")
    print("=" * 60)

    # æ¸¬è©¦æœ‰æ•ˆçš„é ˜åŸŸID
    valid_ids = ["high_dimensional", "time_series", "classification"]
    for domain_id in valid_ids:
        assert validate_domain_id(domain_id), f"{domain_id} æ‡‰è©²æ˜¯æœ‰æ•ˆçš„"
        print(f"âœ… {domain_id} é©—è­‰é€šé")

    # æ¸¬è©¦ç„¡æ•ˆçš„é ˜åŸŸID
    invalid_ids = ["invalid_domain", "nonexistent"]
    for domain_id in invalid_ids:
        assert not validate_domain_id(domain_id), f"{domain_id} æ‡‰è©²æ˜¯ç„¡æ•ˆçš„"
        print(f"âœ… {domain_id} æ­£ç¢ºè­˜åˆ¥ç‚ºç„¡æ•ˆ")

    return True


def test_3_load_methods_metadata():
    """æ¸¬è©¦æ–¹æ³• metadata è¼‰å…¥"""
    print("\n" + "=" * 60)
    print("æ¸¬è©¦ 3: è¼‰å…¥æ–¹æ³• metadata")
    print("=" * 60)

    methods_metadata = load_all_methods_metadata()

    assert len(methods_metadata) > 0, "æ‡‰è©²æœ‰è‡³å°‘ä¸€å€‹æ–¹æ³•"

    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(methods_metadata)} å€‹æ–¹æ³•çš„ metadata")

    # æª¢æŸ¥æ¯å€‹æ–¹æ³•æ˜¯å¦æœ‰ domains æ¬„ä½
    for method_id, metadata in methods_metadata.items():
        domains = metadata.get("domains", [])
        print(f"   - {method_id}: {len(domains)} å€‹é ˜åŸŸæ¨™è¨˜")

        # æª¢æŸ¥ domains æ ¼å¼
        for domain_info in domains:
            assert "domain_id" in domain_info, f"{method_id} çš„é ˜åŸŸæ‡‰æœ‰ domain_id"
            assert "weight" in domain_info, f"{method_id} çš„é ˜åŸŸæ‡‰æœ‰ weight"
            assert "relevance" in domain_info, f"{method_id} çš„é ˜åŸŸæ‡‰æœ‰ relevance"

    return True


def test_4_get_methods_by_domain():
    """æ¸¬è©¦æ ¹æ“šé ˜åŸŸæŸ¥è©¢æ–¹æ³•"""
    print("\n" + "=" * 60)
    print("æ¸¬è©¦ 4: æ ¹æ“šé ˜åŸŸæŸ¥è©¢æ–¹æ³•")
    print("=" * 60)

    # æ¸¬è©¦æŸ¥è©¢é«˜ç¶­åº¦é ˜åŸŸçš„æ–¹æ³•
    high_dim_methods = get_methods_by_domain("high_dimensional", min_weight=0.5)
    print(f"âœ… é«˜ç¶­åº¦é ˜åŸŸæœ‰ {len(high_dim_methods)} å€‹æ–¹æ³•")

    for method in high_dim_methods:
        print(f"   - {method['method_id']} (æ¬Šé‡: {method['weight']}, {method['relevance']})")

    # æ¸¬è©¦æŸ¥è©¢åˆ†é¡é ˜åŸŸçš„æ–¹æ³•
    classification_methods = get_methods_by_domain("classification", min_weight=0.5)
    print(f"âœ… åˆ†é¡é ˜åŸŸæœ‰ {len(classification_methods)} å€‹æ–¹æ³•")

    for method in classification_methods:
        print(f"   - {method['method_id']} (æ¬Šé‡: {method['weight']}, {method['relevance']})")

    return True


def test_5_recommend_without_gpt():
    """æ¸¬è©¦æ¨è–¦ç³»çµ±ï¼ˆä¸ä½¿ç”¨ GPTï¼Œåƒ…æ¸¬è©¦çµæ§‹ï¼‰"""
    print("\n" + "=" * 60)
    print("æ¸¬è©¦ 5: æ¨è–¦ç³»çµ±çµæ§‹æ¸¬è©¦ï¼ˆä¸ä½¿ç”¨ GPTï¼‰")
    print("=" * 60)

    # æ³¨æ„ï¼šé€™å€‹æ¸¬è©¦ä¸æœƒçœŸçš„å‘¼å« GPT
    # å› ç‚ºéœ€è¦ OPENAI_API_KEYï¼Œæ‰€ä»¥æˆ‘å€‘åªæ¸¬è©¦ä¸ä½¿ç”¨ GPT çš„æƒ…æ³

    try:
        result = recommend_methods_by_domains(
            question="æ¸¬è©¦å•é¡Œ",
            df_info=None,
            use_gpt_identification=False
        )

        # æª¢æŸ¥è¿”å›çµæ§‹
        assert "question_domains" in result
        assert "recommended_methods" in result
        assert "reasoning" in result

        print("âœ… æ¨è–¦ç³»çµ±çµæ§‹æ­£ç¢º")
        print(f"   è¿”å›æ¬„ä½: {list(result.keys())}")

    except Exception as e:
        print(f"âš ï¸  æ¸¬è©¦è·³éï¼ˆé æœŸè¡Œç‚ºï¼‰: {e}")

    return True


def test_6_mock_domain_matching():
    """æ¸¬è©¦é ˜åŸŸåŒ¹é…é‚è¼¯ï¼ˆæ¨¡æ“¬ï¼‰"""
    print("\n" + "=" * 60)
    print("æ¸¬è©¦ 6: é ˜åŸŸåŒ¹é…é‚è¼¯ï¼ˆæ¨¡æ“¬ï¼‰")
    print("=" * 60)

    # æ¨¡æ“¬å•é¡Œé ˜åŸŸåˆ†æ•¸
    mock_question_domains = {
        "high_dimensional": 0.9,
        "regression": 0.7
    }

    methods_metadata = load_all_methods_metadata()

    # æ‰‹å‹•è¨ˆç®—åŒ¹é…åˆ†æ•¸ï¼ˆæ¨¡æ“¬æ¨è–¦é‚è¼¯ï¼‰
    for method_id, metadata in methods_metadata.items():
        method_domains = metadata.get("domains", [])
        match_score = 0.0

        for domain_info in method_domains:
            domain_id = domain_info.get("domain_id")
            domain_weight = domain_info.get("weight", 1.0)

            if domain_id in mock_question_domains:
                question_score = mock_question_domains[domain_id]
                contribution = question_score * domain_weight
                match_score += contribution

        if match_score > 0:
            print(f"   {method_id}: åŒ¹é…åˆ†æ•¸ = {match_score:.2f}")

    print("âœ… é ˜åŸŸåŒ¹é…é‚è¼¯æ¸¬è©¦å®Œæˆ")

    return True


def run_all_tests():
    """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
    print("\n" + "ğŸ§ª" * 30)
    print("   é ˜åŸŸæ¨è–¦ç³»çµ±å–®å…ƒæ¸¬è©¦")
    print("ğŸ§ª" * 30)

    tests = [
        ("è¼‰å…¥é ˜åŸŸé…ç½®", test_1_load_domains_config),
        ("é ˜åŸŸIDé©—è­‰", test_2_validate_domain_ids),
        ("è¼‰å…¥æ–¹æ³•metadata", test_3_load_methods_metadata),
        ("æ ¹æ“šé ˜åŸŸæŸ¥è©¢æ–¹æ³•", test_4_get_methods_by_domain),
        ("æ¨è–¦ç³»çµ±çµæ§‹", test_5_recommend_without_gpt),
        ("é ˜åŸŸåŒ¹é…é‚è¼¯", test_6_mock_domain_matching)
    ]

    passed = 0
    failed = 0

    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
        except AssertionError as e:
            print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {test_name}")
            print(f"   éŒ¯èª¤: {e}")
            failed += 1
        except Exception as e:
            print(f"\nâš ï¸  æ¸¬è©¦éŒ¯èª¤: {test_name}")
            print(f"   éŒ¯èª¤: {e}")
            failed += 1

    print("\n" + "=" * 60)
    print("æ¸¬è©¦ç¸½çµ")
    print("=" * 60)
    print(f"âœ… é€šé: {passed}/{len(tests)}")
    print(f"âŒ å¤±æ•—: {failed}/{len(tests)}")

    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼")
        return True
    else:
        print(f"\nâš ï¸  æœ‰ {failed} å€‹æ¸¬è©¦å¤±æ•—")
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
