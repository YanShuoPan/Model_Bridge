# Logistic Regression (邏輯迴歸)

## 概述

邏輯迴歸是一種用於**二元分類問題**的統計方法，儘管名稱中有「迴歸」二字，但它實際上是一種分類模型。它透過邏輯函數（logistic function，也稱 sigmoid 函數）將線性組合的預測值轉換為 0 到 1 之間的機率值。

## 數學原理

### 基本模型

給定預測變數 X₁, X₂, ..., Xₚ 和二元結果變數 Y（0 或 1），邏輯迴歸模型為：

```
P(Y=1|X) = 1 / (1 + e^(-(β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ)))
```

或等價地，對數勝算（log-odds）形式：

```
log(P(Y=1) / P(Y=0)) = β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ
```

### 關鍵概念

1. **勝算（Odds）**: 事件發生的機率與不發生機率的比值
   - Odds = P(Y=1) / P(Y=0)
   - 例如：機率 0.8 對應的勝算為 0.8/0.2 = 4

2. **勝算比（Odds Ratio, OR）**: 某變數改變一單位時，勝算的倍數變化
   - OR = exp(β)
   - OR = 1: 無影響
   - OR > 1: 增加結果發生的勝算
   - OR < 1: 減少結果發生的勝算

3. **邏輯函數**: 將實數映射到 (0, 1) 區間的 S 型曲線

## 何時使用

### 適用情境

✅ **應該使用邏輯迴歸的情況：**

- 結果變數是**二元的**（如：是/否、成功/失敗、存活/死亡）
- 需要**了解各因素的影響**及其顯著性
- 希望模型具有**良好的可解釋性**
- 想要獲得**機率預測**而非僅是類別
- 資料量中等，變數不會太多（避免過度配適）

### 實際應用範例

1. **醫療健康**
   - 疾病風險預測（如：糖尿病、心血管疾病）
   - 治療效果評估
   - 患者存活率預測

2. **商業金融**
   - 客戶流失預測（churn prediction）
   - 信用評分與違約風險
   - 行銷活動回應預測

3. **社會科學**
   - 投票行為分析
   - 教育成就預測
   - 政策支持度研究

4. **數位行銷**
   - 廣告點擊率預測
   - 網站轉換率優化
   - 電子郵件開信率分析

## 假設與限制

### 模型假設

使用邏輯迴歸前，應檢查以下假設：

1. **結果變數為二元**: Y 必須是 0/1、是/否等二元類別
2. **觀測獨立性**: 每個觀測值應該相互獨立（非重複測量）
3. **線性關係**: 自變數與對數勝算（log-odds）呈線性關係
4. **無多元共線性**: 預測變數之間不應高度相關
5. **足夠樣本量**: 經驗法則：每個預測變數至少需要 10-15 個「事件」

### 主要限制

⚠️ **邏輯迴歸的限制：**

- **線性假設**: 預設對數勝算與自變數為線性關係，無法自動捕捉複雜的非線性效應
  - 解決方法：加入多項式項、交互作用項，或使用非線性方法（如隨機森林）

- **敏感性**: 對離群值和極端影響點較敏感
  - 解決方法：數據清理、影響點診斷、穩健迴歸方法

- **完全分離問題**: 當某變數能完美預測結果時會導致估計失敗
  - 解決方法：使用懲罰邏輯迴歸（如 Ridge, Lasso）

- **類別不平衡**: 當一類樣本遠多於另一類時，模型可能偏向多數類
  - 解決方法：重新抽樣、調整決策閾值、使用加權

## 結果解釋

### 係數解讀

假設我們建立了一個客戶流失預測模型：

```
log(P(流失) / P(不流失)) = -2.5 + 0.8×合約月數 - 1.2×滿意度分數
```

**解讀方式：**

1. **原始係數（log-odds scale）**:
   - 合約月數係數 = 0.8: 每增加一個月，流失的對數勝算增加 0.8
   - 滿意度係數 = -1.2: 滿意度每增加 1 分，流失的對數勝算減少 1.2

2. **勝算比（Odds Ratio）**:
   - 合約月數 OR = exp(0.8) = 2.23
     → 每增加一個月合約，流失的勝算增加為 2.23 倍
   - 滿意度 OR = exp(-1.2) = 0.30
     → 滿意度每增加 1 分，流失的勝算降低為原本的 30%（或減少 70%）

3. **機率計算**:
   - 若某客戶合約 6 個月、滿意度 8 分：
   - log-odds = -2.5 + 0.8×6 - 1.2×8 = -2.5 + 4.8 - 9.6 = -7.3
   - P(流失) = 1 / (1 + e^7.3) ≈ 0.0007 (0.07%)

### 模型評估指標

1. **準確率（Accuracy）**: 正確預測的比例
   - 適用於類別平衡的情況
   - 類別不平衡時可能誤導

2. **ROC AUC**: 衡量模型區分兩類的能力
   - 0.5: 隨機猜測
   - 0.7-0.8: 可接受
   - 0.8-0.9: 良好
   - > 0.9: 優秀

3. **精確率（Precision）**: 預測為正類中實際為正的比例
   - 當誤報（False Positive）代價高時重要

4. **召回率（Recall）**: 實際為正類中被正確預測的比例
   - 當漏報（False Negative）代價高時重要

5. **F1 分數**: 精確率與召回率的調和平均
   - 平衡兩者的綜合指標

### 決策閾值選擇

預設閾值通常為 0.5，但應根據實際需求調整：

- **降低閾值（如 0.3）**: 增加召回率，減少漏報
  - 適用於：疾病篩檢、詐欺偵測（寧可錯殺，不可錯放）

- **提高閾值（如 0.7）**: 增加精確率，減少誤報
  - 適用於：精準行銷、稀缺資源分配（寧可錯放，不可錯殺）

## 實作考量

### 數據準備

1. **類別變數編碼**:
   - 二元變數: 轉為 0/1
   - 多類別變數: 使用 one-hot encoding 或虛擬變數

2. **數值變數標準化**:
   - 可選但建議，特別是使用正規化時
   - 方便比較不同變數的係數大小

3. **處理缺失值**:
   - 刪除、填補（均值/中位數/模型預測）
   - 或建立「缺失」指示變數

### 模型診斷

1. **檢查多元共線性**: 計算 VIF（Variance Inflation Factor）
   - VIF > 10 表示嚴重共線性

2. **影響點分析**: Cook's distance, DFBETA
   - 找出對模型影響過大的觀測值

3. **殘差分析**: 檢查模型配適度
   - Deviance residuals
   - Pearson residuals

4. **模型比較**: 使用 AIC、BIC 比較不同模型

### 改進策略

1. **特徵工程**:
   - 加入交互作用項（如：年齡 × 性別）
   - 非線性轉換（多項式、對數、平方根）
   - 創建組合特徵

2. **正規化方法**:
   - Ridge (L2): 處理共線性
   - Lasso (L1): 自動變數選擇
   - Elastic Net: 結合兩者

3. **處理不平衡數據**:
   - 過採樣少數類（SMOTE）
   - 欠採樣多數類
   - 調整類別權重
   - 使用適當的評估指標（如 F1, AUROC）

## 與其他方法比較

| 方法 | 可解釋性 | 非線性能力 | 計算成本 | 適用情境 |
|------|---------|-----------|---------|---------|
| **邏輯迴歸** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | 需要解釋、線性關係 |
| 決策樹 | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 非線性、變數互動 |
| 隨機森林 | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | 複雜關係、高準確度 |
| XGBoost | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | 競賽、最高準確度 |
| 神經網路 | ⭐ | ⭐⭐⭐⭐⭐ | ⭐ | 大數據、複雜模式 |
| 樸素貝葉斯 | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | 文本分類、快速原型 |

**選擇建議**:
- 需要**解釋性** → 邏輯迴歸、決策樹
- 需要**高準確度** → 隨機森林、XGBoost
- **小數據集** → 邏輯迴歸、樸素貝葉斯
- **大數據、複雜關係** → 神經網路
- **初步探索** → 從邏輯迴歸開始，再嘗試複雜方法

## 最佳實踐清單

✅ **實作檢查清單**:

- [ ] 確認結果變數為二元
- [ ] 檢查並處理缺失值
- [ ] 檢查類別是否平衡
- [ ] 編碼類別變數
- [ ] 考慮變數標準化
- [ ] 分割訓練集/測試集
- [ ] 配適模型
- [ ] 檢查係數符號是否合理
- [ ] 評估模型表現（多種指標）
- [ ] 繪製 ROC 曲線
- [ ] 進行模型診斷
- [ ] 選擇適當的決策閾值
- [ ] 在測試集上驗證
- [ ] 撰寫清晰的結果報告

## 進階主題

### 多類別邏輯迴歸

當結果有 3 類以上時：
- **One-vs-Rest (OvR)**: 對每一類建立二元模型
- **Multinomial Logistic Regression**: 直接建立多類別模型
- **Softmax Regression**: 神經網路中的多類別輸出層

### 貝氏邏輯迴歸

使用貝氏推論框架：
- 可納入先驗知識
- 提供參數的不確定性估計
- 處理小樣本問題

### 分層/混合效應邏輯迴歸

處理分組或重複測量數據：
- 固定效應 + 隨機效應
- 考慮組內相關性
- 適用於多層次數據結構

## 學習資源

### 線上課程
- Coursera: "Machine Learning" by Andrew Ng
- DataCamp: "Logistic Regression in Python"

### 書籍
- Hosmer & Lemeshow: "Applied Logistic Regression" (經典教材)
- James et al.: "An Introduction to Statistical Learning" (第4章)

### 工具與套件
- **Python**: scikit-learn, statsmodels
- **R**: glm(), glmnet, caret
- **視覺化**: matplotlib, seaborn, plotly

## 總結

邏輯迴歸是一個強大且廣泛使用的分類方法，特別適合：
- 需要**解釋模型**如何做出預測
- 想要了解**各因素的影響大小**
- 建立**可部署的風險評分系統**

雖然它假設線性關係且無法自動處理複雜互動，但其簡潔性、可解釋性和計算效率使其成為許多實務問題的首選方法。在實際應用中，建議先從邏輯迴歸開始建立基準模型，再視需要嘗試更複雜的方法。

---

**相關方法**: [Probit Regression](../probit_regression/description.md) | [Random Forest](../random_forest/description.md) | [Lasso Logistic Regression](../lasso_logistic/description.md)

**返回**: [方法列表](../../README.md)
